{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e41a75ef",
      "metadata": {},
      "source": [
        "# ReversingLabs SDK Advanced Search \n",
        "\n",
        "This notebook demonstrates how to use the ReversingLabs SDK to search and analyze samples using the `AdvancedSearch` and `AdvancedActions` classes. \n",
        "The Advanced Search enables users to filter samples by search criteria submitted in a query string. The script focuses on specific URI and/or file type, wide range of search keywords is available, and they can be combined using search operators to build advanced queries.\n",
        "Advanced Actions is a class containing advanced and combined actions utilizing various different classes such as Static analysis (TCA-0104) and Dynamic analysis (TCA-0106). \n",
        "Combined together, client can have a comprehensive enriched report, providing single URL, File type or any other supported filter value.\n",
        "\n",
        "For a similar implementation reference, see the [ReversingLabs SDK Cookbook - TitaniumCloud Search Notebook](https://alt-gitlab.rl.lan/integrations/sdk/reversinglabs-sdk-cookbook/-/blob/main/TitaniumCloud/search.ipynb?ref_type=heads)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8824c3d",
      "metadata": {},
      "source": [
        "#  1. Importing the required classes\n",
        "First, we will import the required API classes from the ticloud module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ab69c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ReversingLabs.SDK.helper import *\n",
        "from ReversingLabs.SDK.ticloud import AdvancedSearch, AdvancedActions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ab89cf",
      "metadata": {},
      "source": [
        "#  2. Loading the credentials\n",
        "\n",
        "- Credentials\n",
        "Credentials are loaded from a local file instead of being written here in plain text.\n",
        "To learn how to creat the credentials file, see the **Storing and using the credentials** section in the [README file](./README.md)\n",
        "Next, we will load our TitaniumCloud credentials from the local ticloud_credentials.json file.\n",
        "NOTE: Instead of doing this step, you can paste your credentials while creating the Python object in the following step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ed0816",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Configuration\n",
        "# ---------------------------------------------------\n",
        "SERVER = \"<server>\"\n",
        "USERNAME = \"username\"\n",
        "PASSWORD = \"password\"\n",
        "USER_AGENT= \"ReversingLabs SDK Cookbook v2.9.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5beb2d8",
      "metadata": {},
      "source": [
        "# 3. Filter query string\n",
        "This code block defines a Python dictionary named QUERY_STRING that sets up the parameters for an API query to the ReversingLabs platform. When running this in a Jupyter Notebook, it forms the basis for the search request by specifying filters, pagination, and the desired response format.\n",
        "Addiditonal options available here: https://docs.reversinglabs.com/SpectraIntelligence/API/MalwareHunting/tca-0320"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791783de",
      "metadata": {},
      "outputs": [],
      "source": [
        "QUERY_STRING = 'firstseen:[2025-02-20T00:00:00Z TO *] classification:[malicious, suspicious] filetype:EXE uri:\"https://api.telegram.org/bot*\" size:[0 TO *]'\n",
        "# Example for URI: \"https://api.telegram.org/bot*\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956d0a60",
      "metadata": {},
      "source": [
        "# 4. Extract URL prefix from a string query\n",
        "URL Prefix Extraction Function\n",
        "- Extracts the URL prefix from the query string using regex\n",
        "- Helps identify which URLs to look for in the enriched data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbdf8739",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_url_prefix_from_string_query(query_string):\n",
        "    uri_pattern = r'uri:\"?([^\\s\"]*)'\n",
        "    match = re.search(uri_pattern, query_string)\n",
        "    if match:\n",
        "        url_pattern = match.group(1)\n",
        "        # Remove the wildcard asterisk if present\n",
        "        return url_pattern.replace(\"*\", \"\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c3e299",
      "metadata": {},
      "source": [
        "# 5. Recursive Search Function\n",
        "- Searches deeply through nested JSON data from the enriched report\n",
        "- Finds all URLs that match the prefix pattern from the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067c7c95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def recursive_search_for_urls(obj, prefix):\n",
        "    found = []\n",
        "    if isinstance(obj, dict):\n",
        "        for key, value in obj.items():\n",
        "            found.extend(recursive_search_for_urls(value, prefix))\n",
        "    elif isinstance(obj, list):\n",
        "        for item in obj:\n",
        "            found.extend(recursive_search_for_urls(item, prefix))\n",
        "    elif isinstance(obj, str):\n",
        "        if obj.startswith(prefix):\n",
        "            found.append(obj)\n",
        "    return found"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7e378c",
      "metadata": {},
      "source": [
        "# 6. Main Execution Function\n",
        "- Performs the search using the ReversingLabs SDK\n",
        "- Processes each sample to extract basic information\n",
        "- Attempts to enrich each sample and extract relevant URLs\n",
        "- Handles errors gracefully, continuing even if enrichment fails for some samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39a9c90",
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    query_string = QUERY_STRING\n",
        "    url_prefix = extract_url_prefix_from_string_query(query_string)\n",
        "    \n",
        "    search_client = AdvancedSearch(\n",
        "        host=SERVER,\n",
        "        username=USERNAME,\n",
        "        password=PASSWORD,\n",
        "        verify=True,\n",
        "        proxies=None,\n",
        "        user_agent=\"USER_AGENT\",\n",
        "        allow_none_return=False\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        results = search_client.search_aggregated(\n",
        "            query_string=query_string,\n",
        "            sorting_criteria=\"firstseen\",\n",
        "            sorting_order=\"desc\",\n",
        "            max_results=100,\n",
        "            records_per_page=100\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error during search: {e}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Total samples returned: {len(results)}\")\n",
        "    if not results:\n",
        "        print(\"No samples found.\")\n",
        "        return\n",
        "\n",
        "    actions = AdvancedActions(\n",
        "        host=SERVER,\n",
        "        username=USERNAME,\n",
        "        password=PASSWORD,\n",
        "        verify=True,\n",
        "        proxies=None,\n",
        "        user_agent=\"USER_AGENT\",\n",
        "        allow_none_return=False\n",
        "    )\n",
        "\n",
        "    minimal_results = []\n",
        "    enrichment_success_count = 0\n",
        "    urls_found_count = 0\n",
        "    \n",
        "    for sample in results:\n",
        "        sha1 = sample.get(\"sha1\")\n",
        "        if not sha1:\n",
        "            continue\n",
        "\n",
        "        sample_type = sample.get(\"sampletype\") or sample.get(\"filetype\")\n",
        "        minimal_data = {\n",
        "            \"hashes\": {\n",
        "                \"sha1\": sample.get(\"sha1\"),\n",
        "                \"sha256\": sample.get(\"sha256\", \"\"),\n",
        "                \"md5\": sample.get(\"md5\", \"\")\n",
        "            },\n",
        "            \"first_seen\": sample.get(\"firstseen\"),\n",
        "            \"last_seen\": sample.get(\"lastseen\"),\n",
        "            \"sampletype\": sample_type,\n",
        "            \"file_size\": sample.get(\"size\"),\n",
        "            \"classification\": sample.get(\"classification\"),\n",
        "            \"threatname\": sample.get(\"threatname\"),\n",
        "            \"extracted_urls\": []\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            enriched_report = actions.enriched_file_analysis(sha1)\n",
        "            found_urls = recursive_search_for_urls(enriched_report, url_prefix)\n",
        "            if found_urls:\n",
        "                minimal_data[\"extracted_urls\"] = list(set(found_urls))  # Deduplicate\n",
        "                urls_found_count += 1\n",
        "            enrichment_success_count += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error enriching sample {sha1}: {e}\")\n",
        "        \n",
        "        minimal_results.append(minimal_data)\n",
        "\n",
        "    print(f\"Found URLs in {urls_found_count} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f59c5f7",
      "metadata": {},
      "source": [
        "# 7. Results Processing and output\n",
        "- Groups samples by the extracted URLs\n",
        "- Creates a default group if no URLs are found\n",
        "- Builds the final report structure \n",
        "- Writes the grouped results to a JSON file\n",
        "- Provides summary statistics on the console"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5680ca67",
      "metadata": {},
      "outputs": [],
      "source": [
        "    url_groups = {}\n",
        "    for sample in minimal_results:\n",
        "        for url in sample.get(\"extracted_urls\", []):\n",
        "            if url not in url_groups:\n",
        "                url_groups[url] = []\n",
        "            url_groups[url].append(sample)\n",
        "\n",
        "    if not url_groups and minimal_results:\n",
        "        print(\"No URLs found in any samples. Creating a default group for all samples.\")\n",
        "        default_url = f\"{url_prefix}[no_specific_url_found]\"\n",
        "        url_groups[default_url] = minimal_results\n",
        "\n",
        "    grouped_output = {\"urls\": []}\n",
        "    for url, samples in url_groups.items():\n",
        "        hashes = [sample[\"hashes\"][\"sha1\"] for sample in samples]\n",
        "        \n",
        "        grouped_output[\"urls\"].append({\n",
        "            \"value\": url,\n",
        "            \"hashes\": hashes,\n",
        "            \"samples\": samples\n",
        "        })\n",
        "    output_file = \"report.json\"\n",
        "    try:\n",
        "        with open(output_file, \"w\") as f:\n",
        "            json.dump(grouped_output, f, indent=2)\n",
        "        print(f\"Grouped report written to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(\"Error exporting report:\", e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
