{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d4f3815a06232d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# YARA retro hunt with timegating on the Spectra Analyze\n",
    "This notebook contains an example of how to use the ReversingLabs SDK \n",
    "to add a YARA ruleset, start a retrohunt and filter output by time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd412eb8c413f8c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Used Spectra Analyze methods\n",
    "- **create_or_update_yara_ruleset**\n",
    "- **enable_or_disable_yara_ruleset**\n",
    "- **start_or_stop_yara_local_retro_scan**\n",
    "- **get_yara_local_retro_scan_status**\n",
    "- **get_yara_ruleset_matches_v2**\n",
    "- **get_summary_report_v2**\n",
    "- **delete_yara_ruleset**\n",
    "\n",
    "### Credentials\n",
    "Credentials are loaded from a local file instead of being written here in plain text.\n",
    "To learn how to create credentials file, see the **Storing and using credentials** section in the [README file](./README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266267ddaa81c3f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Time gated Retro Hunt with Spectra Analyze\n",
    "The Retro Hunt concept is similar for the Spectra Analyze and TiCloud although with some differences.\n",
    "In this example we will be using the Spectra Analyze local Yara Retro Hunt to illustrate these differences.\n",
    "\n",
    "The first code block is almost identical to its TiCloud counterpart.\n",
    "Note however that, depending on your Spectra Analyze certificate setup, you might need to set the `verify` parameter to `False`.\n",
    "The Spectra Analyze ReversingLabs SDK module uses only one status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47610db2afb0a55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import islice\n",
    "import dateutil\n",
    "import datetime\n",
    "\n",
    "from ReversingLabs.SDK.a1000 import A1000\n",
    "\n",
    "CREDENTIALS = json.load(open(\"credentials.json\"))\n",
    "HOST = CREDENTIALS.get(\"a1000\").get(\"a1000_url\")\n",
    "TOKEN = CREDENTIALS.get(\"a1000\").get(\"token\")\n",
    "USER_AGENT = json.load(open('../user_agent.json'))[\"user_agent\"]\n",
    "\n",
    "# Set the verify parameter to False if your Spectra Analyze instance doesn't have a valid CA certificate\n",
    "a1000 = A1000(\n",
    "    host=HOST,\n",
    "    token=TOKEN,\n",
    "    verify=True,\n",
    "    user_agent=USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e21ac0232c4670",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The ruleset used in this example is the same one from before.\n",
    "Since your Spectra Analyze instance will most certainly have a smaller sample collection to work with\n",
    "you should consider changing this rule to target samples from your collection.\n",
    "A before we create the ruleset on the Spectra Analyze instance without using the `ticloud` parameter.\n",
    "If your instance is set up with the TiCloud integration you may explore the TiCloud YARA Retro Hunt using the Spectra Analyze API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7ccecbfa417df",
   "metadata": {},
   "outputs": [],
   "source": [
    "RULESET_NAME = \"Cookbook_NSIS_Installer\"\n",
    "RULESET_CONTENT = f\"\"\"\n",
    "import \"pe\"\n",
    "\n",
    "rule {RULESET_NAME}\n",
    "{{\n",
    "\t/* a */\n",
    "    meta:\n",
    "        offset = \"0x4031d1\"\n",
    "        examplar = \"4313d352e0dafd1f22b6517126a655cae3b444fa758d2845eddfbe72f24f7bdd\"\n",
    "    strings:\n",
    "        $op = {{\n",
    "            81[2-3]efbeadde [2-6]\n",
    "            81[2-3]496e7374 [2-6]\n",
    "            81[2-3]736f6674 [2-6]\n",
    "            81[2-3]4e756c6c\n",
    "        }}\n",
    "        $nsis = \"\\\\xef\\\\xbe\\\\xad\\\\xdeNullsoftInst\"\n",
    "    condition:\n",
    "        pe.sections[pe.section_index(@op)].characteristics & (pe.SECTION_MEM_READ | pe.SECTION_MEM_EXECUTE) and\n",
    "        $nsis in (pe.overlay.offset..pe.overlay.offset+pe.overlay.size)\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "response = a1000.create_or_update_yara_ruleset(RULESET_NAME, RULESET_CONTENT, ticloud=False)\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aad1f3eed5099d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once the rule is present on the appliance we may start the local Retro Hunt.\n",
    "We can check the Retro Hunt status to see the start and end timestamps and the hunt progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd2a5c41a6c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = a1000.enable_or_disable_yara_ruleset(True, RULESET_NAME)\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=1))\n",
    "\n",
    "response = a1000.start_or_stop_yara_local_retro_scan(\"START\")\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=1))\n",
    "\n",
    "response = a1000.get_yara_local_retro_scan_status()\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b5c8d81f6fcc3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The SDK method `get_yara_ruleset_matches_v2()` allows us to get a paginated view of samples\n",
    "which match our rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd703467697ca7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = a1000.get_yara_ruleset_matches_v2(RULESET_NAME, page=\"1\", page_size=\"10\")\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json()[\"results\"], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f19fb602a35b502",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To make the consumption of matched samples more convenient we will define a generator function.\n",
    "The structure of the generator `a1000_complete_feed()` is the same as in the TiCloud example.\n",
    "We use the matches function to get the first page of the matches feed.\n",
    "The `response.next` property holds a full url of the next page.\n",
    "To keep things simple we will just check for the presence of this property and assume that the page index increases by 1.\n",
    "Depending on your use case you may wish to inspect/alter this url before fetching the next page.\n",
    "\n",
    "Any match found in the response is yielded to the caller.\n",
    "Once all are passed to the caller the generator fetches the next page until all have been checked.\n",
    "\n",
    "The output of the cell below shows the first match of the feed.\n",
    "Here we control the number of elements using the limit parameter of the generator.\n",
    "\n",
    "**NOTE:** If the rule is cloud enabled and the cloud retro hunt started the feed might contain cloud samples.\n",
    "These samples may only have the sha1 hash present for the sample. You may not be able to pull them onto the appliance for analysis.\n",
    "These are just differences between local and cloud retro hunt matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d49d9938f21d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a1000_complete_feed(limit=None):\n",
    "    produced = 0\n",
    "    next_page = 1\n",
    "    while next_page:\n",
    "        parsed = a1000.get_yara_ruleset_matches_v2(RULESET_NAME, page=str(next_page), page_size=\"100\").json()\n",
    "        # NOTE that Spectra Analyze API returns a direct link but the page pointer is an incrementing integer\n",
    "        next_page = next_page + 1 if parsed.get(\"next\") else None\n",
    "        for e in parsed.get(\"results\", []):\n",
    "            produced += 1\n",
    "            yield e\n",
    "            if limit and produced >= limit:\n",
    "                return\n",
    "\n",
    "match = list(a1000_complete_feed(1))[0]\n",
    "print(json.dumps(match, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc3d0dacf4801d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Since this example uses only the local Retro Hunt only local samples will be present in the feed.\n",
    "This means we can use `get_summary_report_v2()` to enrich found matches.\n",
    "Here is an example of that report for the first match in the feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172a9dbcbfc29d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = a1000.get_summary_report_v2(\n",
    "    match[\"sha256\"], \n",
    "    skip_reanalysis=True, \n",
    "    include_networkthreatintelligence=False, \n",
    "    fields=[\"sha256\", \"local_first_seen\", \"local_last_seen\"]\n",
    ")\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f91d0abc048cf7e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Spectra Analyze allows us to retrieve the summary reports in bulk.\n",
    "So as with the TiCloud example we will create the helper function `batched_1000()`.\n",
    "\n",
    "We then define the `group_with_summary()` function which will iterate over the matches feed in batches.\n",
    "Note that we request only the filed from the summary report which are interesting to us.\n",
    "These are: sha256, local_first_seen, local_last_seen and classification.\n",
    "We will use these fields to filter out samples which are not of interest from the feed.\n",
    "\n",
    "Again as with the previous example we print out some of the matches and their summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc6540614c2523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_a1000(iterable, n):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        batch = list(islice(it, n))\n",
    "        if not batch:\n",
    "            return\n",
    "        yield batch\n",
    "        \n",
    "        \n",
    "def group_with_summary(retro_matches):\n",
    "    for batch in batched_a1000(retro_matches, 100):\n",
    "        sha256_to_match = {e[\"sha256\"]: e for e in batch}\n",
    "        batch_summary = a1000.get_summary_report_v2(\n",
    "            list(sha256_to_match.keys()), \n",
    "            skip_reanalysis=True, \n",
    "            include_networkthreatintelligence=False, \n",
    "            fields=[\"sha256\", \"local_first_seen\", \"local_last_seen\", \"classification\"]\n",
    "        ).json()\n",
    "        for summary in batch_summary[\"results\"]:\n",
    "            yield sha256_to_match[summary[\"sha256\"]], summary\n",
    "\n",
    "print(json.dumps(list(group_with_summary(a1000_complete_feed(3))), indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91304e36cded73",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Our filter function will be `a1000_filter()` which accepts three filter parameters.\n",
    "The `earlier` and `later` are two (timezone aware) `datetime` objects.\n",
    "The filter requires that the matched sample's first and last seen time be between these values.\n",
    "The third parameter is the classifications list which contains classification which we are interested in.\n",
    "\n",
    "Finally, we can compose a stream of matches and their summaries by using the functions and generators defined so far.\n",
    "We will consume this stream using `list` and `islice` to create the list of at most 10 match and summary pairs.\n",
    "We will then print out the first element of that list.\n",
    "\n",
    "Note that the 10 element stream size is to keep the example simple.\n",
    "You may wish to further enrich the stream by reusing the `get_summary_report_v2()` method with additional fields.\n",
    "Or you may wish to send the samples from the stream for reanalysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17dc1553132823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a1000_time_parse(dt):\n",
    "    return dateutil.parser.isoparse(dt)\n",
    "\n",
    "def a1000_filter(earlier, later, classifications):\n",
    "    def inner(match_summary):\n",
    "        _, summary = match_summary\n",
    "        fs = a1000_time_parse(summary[\"local_first_seen\"])\n",
    "        ls = a1000_time_parse(summary[\"local_last_seen\"])\n",
    "        classification = summary[\"classification\"].lower()\n",
    "        return earlier <= fs <= later and earlier <= ls <= later and classification in classifications\n",
    "    return inner\n",
    "\n",
    "\n",
    "now = datetime.datetime.now().replace(tzinfo=datetime.timezone.utc)\n",
    "year_ago = now - datetime.timedelta(weeks=54)\n",
    "\n",
    "stream = filter(\n",
    "    a1000_filter(year_ago, now, [\"malicious\"]), \n",
    "    group_with_summary(a1000_complete_feed())\n",
    ")\n",
    "consumed = list(islice(stream, 10))\n",
    "\n",
    "print(json.dumps(consumed[0], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f628725e129a467",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To round off the example we will clean up the appliance by doing the following:\n",
    "- stop the local Retro Hunt\n",
    "- check its status\n",
    "- disable the ruleset we created at the start of the example\n",
    "- delete the ruleset\n",
    "\n",
    "**NOTE:** Depending on the size of your sample collection the retro hunt may already be finished.\n",
    "This means that the STOP function call will return a 412 error code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd4dc8c13d510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = a1000.start_or_stop_yara_local_retro_scan(\"STOP\")\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=1))\n",
    "\n",
    "response = a1000.get_yara_local_retro_scan_status()\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=1))\n",
    "\n",
    "response = a1000.enable_or_disable_yara_ruleset(False, RULESET_NAME)\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=1))\n",
    "\n",
    "response = a1000.delete_yara_ruleset(RULESET_NAME)\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=1))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
