{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b6492c5b965d24",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Threat hunting workflow\n",
    "This notebook contains an example of how to use the ReversingLabs SDK to hunt for\n",
    "samples which fall out of the YARA Retro Hunt timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636f835326dcfbc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Used titaniumCloud classes\n",
    "- **YARARetroHunting** (*TCA-0319 - Start/Cancel YARA Retro Hunt*)\n",
    "- **YARAHunting** (*TCA-0303 - YARA Matches Feed*)\n",
    "- **AdvancedSearch** (*TCA-0320 - Advanced Search*)\n",
    "- **RHA1FunctionalSimilarity** (*TCA-0301 - Group By RHA1 Single Query*)\n",
    "- **ReanalyzeFile** (*TCA-0205 - Re-Analyze File Single Query*)\n",
    "- **DynamicAnalysis** (*TCA-0106 - File Dynamic Analysis Report*)\n",
    "- **FileReputation** (*TCA-0101 - File reputation (Malware Presence)*)\n",
    "- **FileAnalysis** (*TCA-0104 - File Analysis (RLDATA)*)\n",
    "\n",
    "### Credentials\n",
    "Credentials are loaded from a local file instead of being written here in plain text.\n",
    "To learn how to create credentials file, see the **Storing and using credentials** section in the [README file](./README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d14aa1043f63a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Threat Hunting Workflow\n",
    "In this notebook we will use the RL SDK python package to look for samples which match our YARA rule reaching as far back as possible.\n",
    "To do this we will have to build a pipeline which will employ various TiCloud APIs.\n",
    "The complete pipeline is show on the following diagram.\n",
    "Each of the elements in the diagram is explained in detail in this notebook.\n",
    "\n",
    "\n",
    "![text](images/pipeline.png \"Pipeline diagram\")\n",
    "\n",
    "\n",
    "The pipeline consists of three distinct stages: A, B and C.\n",
    "The first stage is the setup stage in which we will create our rule on TiCloud and start a Retro Hunt for that rule.\n",
    "Stage B handles processing of samples retrieved using the Advanced Search API.\n",
    "This API is the keystone of our efforts to extend the YARA hunt beyond the reach of a YARA Retro Hunt (90 days).\n",
    "Stage C collects matches from both YARA Retro Hunt feed and regular YARA feed.\n",
    "These matches are then enriched with the static and dynamic reports (if those exist) and stored to local storage.\n",
    "\n",
    "The pipeline has multiple parameters which you may tweak to modify its behaviour.\n",
    "The first two parameters are of course the YARA rule and the Advanced Search query.\n",
    "These will, of course, have the most impact on the collected samples.\n",
    "**NOTE** that you should use search parameters which closely match what the YARA rule is hunting for.\n",
    "There are 4 limit parameters as well.\n",
    "Modifying or removing these limits will impact the performance of the pipeline.\n",
    "**NOTE** to remove a limit simply assign the value `None` to the constant, for example `SEARCH_LIMIT = None`.\n",
    "The primary intention of these limits is to allow users to reduce the number of samples which this pipeline produces.\n",
    "TiCloud has a large number of samples, it may be impractical to pull a lot of reports onto a machine you which you are running this notebook.\n",
    "\n",
    "**NOTE** that this pipeline is kept as simple as possible. There is minimal error handling.\n",
    "We do not employ asynchronous, multithreading or prefetching techniques, any of which may be used to improve the performance of the pipeline.\n",
    "Any sample reports we pull are stored on local storage.\n",
    "The purpose of this pipeline is first and foremost to illustrate how you could enhance your usage of TiCloud APIs.\n",
    "\n",
    "To better explain what we are doing we will use the following image.\n",
    "\n",
    "\n",
    "![text](images/timeline.png \"Timeline\")\n",
    "\n",
    "\n",
    "When a YARA ruleset is created any sample analyzed by TiCloud is matched against it.\n",
    "If the sample matches the ruleset it will be present in the YARA matches feed.\n",
    "We can at any time start a YARA Retro Hunt to try and match any sample which was uploaded to TiCloud in the last 90 days.\n",
    "But if wish to hunt for samples which might match our YARA ruleset and have not appeared on TiCloud in the last 90 days we will have to do a little bit of work.\n",
    "\n",
    "The Advanced Search API allows us to search for samples by any number of parameters.\n",
    "The two most important ones are **firstseen** and **lastseen**.\n",
    "If we create a ruleset and immediately start a Retro Hunt we can take a timestamp and calculate these values.\n",
    "In this notebook we will only look a year into the past, but you may wish to go event further back.\n",
    "The red part of the timeline is limited by the **firstseen** and **lastseen** parameters of the search query.\n",
    "\n",
    "We will take the search results which fall under this timeframe and submit them for reanalysis.\n",
    "Since we created a YARA ruleset before doing so these old samples will be matched against it.\n",
    "\n",
    "The blue segment on the timeline represents the timeframe covered by the YARA Retro Hunt.\n",
    "The green part of the timeline is the period in which we run the pipeline to consume the Retro Hunt and regular YARA feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318c12fe078bf3f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Building the pipeline\n",
    "The first step is to import the modules necessary for our implementation.\n",
    "Here we load the credentials and instantiate the required SDK objects.\n",
    "Additionally, we define a dataclass **PipelineItem** which will simply hold any responses from TiCloud relating to a single sample.\n",
    "This dataclass contains a few helper methods as well.\n",
    "\n",
    "The **\\*_LIMIT** constant are present here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import dateutil\n",
    "import json\n",
    "from pathlib import Path\n",
    "from itertools import islice, chain\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Iterable, List, Union\n",
    "\n",
    "from ReversingLabs.SDK import ticloud\n",
    "from ReversingLabs.SDK.helper import NotFoundError\n",
    "\n",
    "\n",
    "CREDENTIALS = json.load(open('credentials.json'))\n",
    "USERNAME = CREDENTIALS.get(\"ticloud\").get(\"username\")\n",
    "PASSWORD = CREDENTIALS.get(\"ticloud\").get(\"password\")\n",
    "USER_AGENT = json.load(open('../user_agent.json'))[\"user_agent\"]\n",
    "\n",
    "config = {\n",
    "    \"host\": \"https://data.reversinglabs.com\",\n",
    "    \"username\": USERNAME,\n",
    "    \"password\": PASSWORD,\n",
    "    \"user_agent\": USER_AGENT\n",
    "}\n",
    "\n",
    "yara_hunting = ticloud.YARAHunting(**config)\n",
    "yara_retro = ticloud.YARARetroHunting(**config)\n",
    "advanced_search = ticloud.AdvancedSearch(**config)\n",
    "rha1_similarity = ticloud.RHA1FunctionalSimilarity(**config)\n",
    "reanalyze_file = ticloud.ReanalyzeFile(**config)\n",
    "dynamic_analysis = ticloud.DynamicAnalysis(**config)\n",
    "file_reputation = ticloud.FileReputation(**config)\n",
    "static_analysis = ticloud.FileAnalysis(**config)\n",
    "\n",
    "\n",
    "# pipeline generator limits\n",
    "SEARCH_LIMIT = 10000\n",
    "SIMILAR_LIMIT = 10000\n",
    "RETRO_LIMIT = 1000\n",
    "YARA_LIMIT = 1000\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineItem:\n",
    "    yara: Optional[Dict] = None\n",
    "    retro: Optional[Dict] = None\n",
    "    rha1: Optional[Union[str, Dict]] = None\n",
    "    search: Optional[Dict] = None\n",
    "    reputation: Optional[Dict] = None\n",
    "    dynamic: Optional[Dict] = None\n",
    "    static: Optional[Dict] = None\n",
    "    \n",
    "    def sample_available(self):\n",
    "        yara = self.yara.get(\"sample_available\", False) if self.yara else False\n",
    "        retro = self.retro.get(\"sample_available\", False) if self.retro else False\n",
    "        return yara or retro\n",
    "    \n",
    "    def get_sha1(self) -> str:\n",
    "        if self.yara:\n",
    "            return self.yara[\"sha1\"]\n",
    "        if self.retro:\n",
    "            return self.retro[\"sha1\"]\n",
    "        if self.rha1:\n",
    "            return self.rha1[\"sha1\"] if not isinstance(self.rha1, str) else self.rha1\n",
    "        if self.search:\n",
    "            return self.search[\"sha1\"]\n",
    "        if self.reputation:\n",
    "            return self.reputation[\"sha1\"]\n",
    "        raise ValueError(\"Empty PipelineItem\")\n",
    "\n",
    "    def get_firstseen(self) -> Optional[datetime.datetime]:\n",
    "        if isinstance(self.rha1, dict) and self.rha1.get(\"first_seen\"):\n",
    "             # \"first_seen\": \"2022-04-20T23:59:32.825000\",\n",
    "             return dateutil.parser.isoparse(\n",
    "                 self.rha1[\"first_seen\"]\n",
    "             ).replace(tzinfo=datetime.timezone.utc)\n",
    "        if self.search and self.search.get(\"firstseen\"):\n",
    "            #  \"firstseen\": \"2024-03-13T02:38:00Z\",\n",
    "            return dateutil.parser.isoparse(\n",
    "                self.search[\"firstseen\"]\n",
    "            ).replace(tzinfo=datetime.timezone.utc)\n",
    "        if self.reputation and self.reputation.get(\"first_seen\"):\n",
    "            # \"first_seen\": \"2024-01-12T23:00:13\",\n",
    "            return dateutil.parser.isoparse(\n",
    "                self.reputation[\"first_seen\"]\n",
    "            ).replace(tzinfo=datetime.timezone.utc)\n",
    "        return None\n",
    "    \n",
    "    def get_lastseen(self) -> Optional[datetime.datetime]:\n",
    "        if isinstance(self.rha1, dict) and self.rha1.get(\"last_seen\"):\n",
    "             #  \"last_seen\": \"2023-12-29T13:33:39.772000\"\n",
    "             return dateutil.parser.isoparse(\n",
    "                 self.rha1[\"last_seen\"]\n",
    "             ).replace(tzinfo=datetime.timezone.utc)\n",
    "        if self.search and self.search.get(\"lastseen\"):\n",
    "            #  \"lastseen\": \"2024-05-27T14:12:54Z\",\n",
    "            return dateutil.parser.isoparse(\n",
    "                self.search[\"lastseen\"]\n",
    "            ).replace(tzinfo=datetime.timezone.utc)\n",
    "        if self.reputation and self.reputation.get(\"last_seen\"):\n",
    "            #  \"last_seen\": \"2024-06-11T05:09:05\",\n",
    "            return dateutil.parser.isoparse(\n",
    "                self.reputation[\"last_seen\"]\n",
    "            ).replace(tzinfo=datetime.timezone.utc)\n",
    "        return None\n",
    "    \n",
    "    def as_dict(self):\n",
    "        return {\n",
    "            \"yara\": self.yara,\n",
    "            \"retro\": self.retro,\n",
    "            \"rha1\": self.rha1,\n",
    "            \"search\": self.search,\n",
    "            \"reputation\": self.reputation,\n",
    "            \"dynamic\": self.dynamic,\n",
    "            \"static\": self.static,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f1971623a6e7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As mentioned before the first step on our hunt for samples is to create a YARA ruleset on the TiCloud.\n",
    "When a ruleset is created its matches feed is filled automatically with new samples.\n",
    "**NOTE** that trying to recreate a ruleset with the same name will result in a bad request error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81ae3205ec7e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "RULESET_NAME = \"threat_hunting_workflow_NSIS_Installer\"\n",
    "RULESET_CONTENT = f\"\"\"\n",
    "import \"pe\"\n",
    "\n",
    "rule {RULESET_NAME}\n",
    "{{\n",
    "\t/* a */\n",
    "    meta:\n",
    "        offset = \"0x4031d1\"\n",
    "        examplar = \"4313d352e0dafd1f22b6517126a655cae3b444fa758d2845eddfbe72f24f7bdd\"\n",
    "    strings:\n",
    "        $op = {{\n",
    "            81[2-3]efbeadde [2-6]\n",
    "            81[2-3]496e7374 [2-6]\n",
    "            81[2-3]736f6674 [2-6]\n",
    "            81[2-3]4e756c6c\n",
    "        }}\n",
    "        $nsis = \"\\\\xef\\\\xbe\\\\xad\\\\xdeNullsoftInst\"\n",
    "    condition:\n",
    "        pe.sections[pe.section_index(@op)].characteristics & (pe.SECTION_MEM_READ | pe.SECTION_MEM_EXECUTE) and\n",
    "        $nsis in (pe.overlay.offset..pe.overlay.offset+pe.overlay.size)\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "response = yara_hunting.create_ruleset(RULESET_NAME, RULESET_CONTENT)\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json(), indent=1))\n",
    "\n",
    "response = yara_hunting.get_ruleset_info(RULESET_NAME)\n",
    "print(json.dumps(response.json(), indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562469afc4152d4c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**NOTE:** Rule delete API call is provided here for convenience.\n",
    "It is commented out to prevent it being run if you wish to run all the cells automatically.\n",
    "Deleting a rule will stop its Retro Hunt and clear its YARA matches feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1d516d042269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = yara_hunting.delete_ruleset(RULESET_NAME)\n",
    "# print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2a1bd2bec5056",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From this point on any sample analyzed by TiCloud will be matched against the ruleset we just created.\n",
    "Let us now try to match any samples which have already been analyzed by TiCloud.\n",
    "We will do this with a YARA Retro Hunt.\n",
    "We take a timestamp the moment we start the Retro Hunt.\n",
    "This will be our point of reference for timegating found samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7941ffee37edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "retro_started = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc)\n",
    "# response = yara_retro.enable_retro_hunt(RULESET_NAME)\n",
    "# print(json.dumps(response.json(), indent=1))\n",
    "\n",
    "response = yara_retro.start_retro_hunt(RULESET_NAME)\n",
    "print(json.dumps(response.json(), indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9a161b1b858fb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "While the Retro Hunt works in the background we can take a moment to explore how Advanced Search find samples on TiCloud.\n",
    "We use the firstseen and lastseen parameters to limit the number of samples on the input of our pipeline.\n",
    "Here we print out the first search result to see what we are working with.\n",
    "\n",
    "**NOTE** that the search parameters should closely match the content of what the YARA ruleset is searching for.\n",
    "In this example the YARA ruleset is looking for NSIS installers and our search query is looking for sample types\n",
    "which match \"PE/Exe/NSIS\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3e360907bf084",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_FORMAT = \"%Y-%m-%dT%H:%M:%SZ\"\n",
    "year_ago = retro_started - datetime.timedelta(days=365)\n",
    "ninety_days_ago = retro_started - datetime.timedelta(days=90)\n",
    "formatted_y = year_ago.strftime(SEARCH_FORMAT)\n",
    "formatted_nda = ninety_days_ago.strftime(SEARCH_FORMAT)\n",
    "\n",
    "search_query = (f\"sampletype:\\\"PE/Exe/NSIS\\\" AND \"\n",
    "                f\"classification:malicious AND \"\n",
    "                f\"firstseen:[{formatted_y} TO {formatted_nda}] AND \"\n",
    "                f\"lastseen:[{formatted_y} TO {formatted_nda}]\")\n",
    "response = advanced_search.search(search_query, page_number=1)\n",
    "print(response.status_code)\n",
    "print(json.dumps(response.json()[\"rl\"][\"web_search_api\"][\"entries\"][0], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0db1bfecb20380a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let us define a generator function which will be the input for our pipeline.\n",
    "Let's call it `complete_search_stream()`.\n",
    "Note that most of the paginated APIs in this example are consumed with such generator functions.\n",
    "In this example we will not be using the sorting capabilities of the search API.\n",
    "But keep in mind that you may improve performance with smart use of these parameters.\n",
    "Notice that the search results are wrapped in the `PipelineItem` dataclass.\n",
    "\n",
    "We can use the standard `itertools.islice()` iterator to limit the number of elements we wish to \n",
    "consume from the generator. This is how our **\\*_LIMIT**s are enforced.\n",
    "\n",
    "As always we print out one of the items from the generator as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f976dd580b657f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_search_stream(query):\n",
    "    next_page = 1\n",
    "    more_pages = True\n",
    "    consumed = 0\n",
    "    while more_pages:\n",
    "        search_page = advanced_search.search(query, page_number=next_page).json()[\"rl\"][\"web_search_api\"]\n",
    "        next_page = search_page.get(\"next_page\")\n",
    "        more_pages = search_page.get(\"more_pages\", False)\n",
    "        entries = search_page[\"entries\"]\n",
    "        for e in entries:\n",
    "            yield PipelineItem(search=e)\n",
    "        consumed += len(entries)\n",
    "        total_count = search_page.get(\"total_count\", \"Unknown\")\n",
    "        print(f\"Consumed {consumed}/{total_count} samples from Advanced Search\")\n",
    "            \n",
    "second_result = list(\n",
    "    islice(complete_search_stream(search_query), 2)\n",
    ")[1].search\n",
    "print(json.dumps(second_result, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aac60e38882f80",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the same vein as the search generator we define a generator for the RHA1 Functional Similarity API.\n",
    "You can see more example of how to use this API in the [Certificate Search](./certificate_search.ipynb) notebook.\n",
    "This generator takes a sample hash and returns a stream of `PipelineItem`s which are funcationally similar to the argument sample.\n",
    "You can read more on the RHA1 hash [here](https://www.reversinglabs.com/technology/reversinglabs-hash-algorithm).\n",
    "\n",
    "Notice that we ignore two types of exceptions.\n",
    "The first one is thrown when the requested sample's type is not supported by the RHA1 API.\n",
    "The second one is thrown when no similar samples are found.\n",
    "In either case the result is an empty generator which may be consumed by client code without issue.\n",
    "\n",
    "**Note** that the return type of the RHA1 API differs based on the value of the `extended` parameter.\n",
    "This is reflected in the type signature of `PipelineItem` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e21f95b1331151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rha1_similar_to_sample(sample_hash, classification=None, extended=False):\n",
    "    next_page = None\n",
    "    has_more_pages = True\n",
    "    while has_more_pages:\n",
    "        try:\n",
    "            similar_result = rha1_similarity.get_similar_hashes(\n",
    "                sample_hash, \n",
    "                extended_results=extended,\n",
    "                page_sha1=next_page, \n",
    "                classification=classification\n",
    "            ).json()[\"rl\"][\"group_by_rha1\"]\n",
    "        except (ValueError, NotFoundError):\n",
    "            # only some sample types support the rha1 similarity check\n",
    "            # a ValueError is thrown in case a sample is not supported\n",
    "            # see Documentation and ticloud.RHA1_TYPE_MAP\n",
    "            return\n",
    "        next_page = similar_result.get(\"next_page_sha1\")\n",
    "        has_more_pages = next_page is not None\n",
    "        for entry in similar_result[\"sha1_list\"]:\n",
    "            yield PipelineItem(rha1=entry)\n",
    "            \n",
    "example_hash = \"00c3ddd93924df51e10585167271e27b4cfb29c5\"\n",
    "rha1_simple = list(islice(rha1_similar_to_sample(example_hash), 1))[0]\n",
    "rha1_extended = list(islice(rha1_similar_to_sample(example_hash, extended=True), 1))[0]\n",
    "print(json.dumps(rha1_simple.rha1, indent=1))\n",
    "print(json.dumps(rha1_extended.rha1, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef05701d71ad61",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that using the `extended=True` parameter of the API significantly slows it down. \n",
    "We use it in our pipeline since it returns the firstseen and lastseen timestamps.\n",
    "If we wanted to we could have added these timestamps to our `PipelineItem`s using the reputation API.\n",
    "\n",
    "While we were busy with setting up our Advanced Search and RHA1 Functionality generator functions\n",
    "the YARA retro hunt has been scanning the samples on TiCloud.\n",
    "Now it's time to set up our Retro Hunt Matches Feed generator function.\n",
    "Let's call it `complete_retro_feed()` just like the example in [Retro Hunt with timegating](./retro_hunt_with_timegating.ipynb) notebook.\n",
    "Although this time we will forgo the limit parameter in favor of `itertools.islice()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4fc3514753236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_retro_feed(start_time):\n",
    "    next_page = str(int(start_time.timestamp()))\n",
    "    while next_page:\n",
    "        raw = yara_retro.yara_retro_matches_feed(\"timestamp\", next_page)\n",
    "        parsed = raw.json()[\"rl\"][\"feed\"]\n",
    "        next_page = str(parsed.get(\"last_timestamp\"))\n",
    "        for e in parsed.get(\"entries\", []):\n",
    "            yield PipelineItem(retro=e)\n",
    "\n",
    "retro_example = list(islice(complete_retro_feed(retro_started), 1))[0]\n",
    "print(json.dumps(retro_example.retro, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee1c1fb5daed12",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We define the regular YARA feed generator the same way we define the Retro Hunt generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59ba6c1acd1bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_yara_feed(start_time):\n",
    "    next_page = str(int(start_time.timestamp()))\n",
    "    while next_page:\n",
    "        raw = yara_hunting.yara_matches_feed(\"timestamp\", next_page)\n",
    "        parsed = raw.json()[\"rl\"][\"feed\"]\n",
    "        next_page = str(parsed.get(\"last_timestamp\"))\n",
    "        for e in parsed.get(\"entries\", []):\n",
    "            yield PipelineItem(yara=e)\n",
    "            \n",
    "yara_example = list(islice(complete_yara_feed(retro_started), 1))[0]\n",
    "print(json.dumps(yara_example.yara, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233af237df7c1d92",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we define some utility functions.\n",
    "The first two are filter functions which return an inner function.\n",
    "We will use these in combination with python's `filter()` remove samples from our pipeline which do not fall under our timeframe.\n",
    "The `batched()` function consumes items from an iterator and returns an iterator of at most `n` elements.\n",
    "We need this function to allow us to use TiCloud bulk queries in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0456066a553b57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lastseen_older_than(limit):\n",
    "    def inner(i: PipelineItem):\n",
    "        lastseen = i.get_lastseen()\n",
    "        return lastseen and lastseen < limit\n",
    "    return inner\n",
    "\n",
    "\n",
    "def firstseen_newer_than(limit):\n",
    "    def inner(i: PipelineItem):\n",
    "        firstseen = i.get_firstseen()\n",
    "        return firstseen and limit <= firstseen\n",
    "    return inner\n",
    "\n",
    "\n",
    "def batched(iterable, n):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        b = list(islice(it, n))\n",
    "        if not b:\n",
    "            return\n",
    "        yield b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599afab4180b676",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The first of our enrichment functions is `group_with_reputation()`.\n",
    "This function takes an iterable of `PipelineItem`s and uses the `batched()` function to gather them in batches of 100.\n",
    "We query the File Reputation API to get the reputations for these batches.\n",
    "A reputation is then added to each `PipelineItem` from the parameter iterable.\n",
    "Each item is then yielded, thus creating a generator of enriched items.\n",
    "\n",
    "In the example we create a `complete_retro_feed()` generator, wrap it in `group_with_reputation()` and consume one element from it.\n",
    "We print two JSON responses, one from the YARA Feed and one from the File Reputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f69bb415f4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_with_reputation(items: Iterable[PipelineItem]):\n",
    "    for candidates in batched(items, 100):\n",
    "        sha1_to_item = {e.get_sha1(): e for e in candidates}\n",
    "        batch_reputation = file_reputation.get_file_reputation(\n",
    "            list(sha1_to_item.keys())\n",
    "        ).json()[\"rl\"]\n",
    "        if batch_reputation.get(\"invalid_hashes\"):\n",
    "            print(\"Invalid hashes in stream\", batch_reputation[\"invalid_hashes\"])\n",
    "        for reputation in batch_reputation[\"entries\"]:\n",
    "            reputation_hash = reputation[\"query_hash\"][\"sha1\"]\n",
    "            i = sha1_to_item[reputation_hash]\n",
    "            i.reputation = reputation\n",
    "            yield i\n",
    "            \n",
    "\n",
    "enriched = list(islice(group_with_reputation(complete_retro_feed(retro_started)), 1))[0]\n",
    "print(json.dumps(enriched.retro, indent=1))\n",
    "print(json.dumps(enriched.reputation, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d379bf0745b8c6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we are one step closer to combining the streams we defined into a pipeline.\n",
    "But first lets go over what we are trying to do:\n",
    "1. We create a nd activate our threat hunting YARA ruleset to hunt new incoming samples\n",
    "2. We start a retro hunt with our YARA ruleset to hunt for samples up to 90 days old\n",
    "3. To use our threat hunting YARA ruleset to hunt for samples older than 90 days we:\n",
    "    - Use the Advanced Search to form a query that will loosely correspond to samples that might be a match for our YARA ruleset for a period older than 90 days\n",
    "    - If hunting for executables, use the RHA1 Functional Similarity API to find samples similar to the YARA and retro YARA ruleset matched files\n",
    "To improve the performance of the pipeline, and to reduce the \"noise\" - i.e. samples we're not interested in, we are filtering RHA1 Advanced Search & Retro YARA source files by defined FS and LS times.\n",
    "\n",
    "In the case of samples found by RHA1 we have the firstseen information (`extend_results=True`).\n",
    "In the case of Advanced Search we can skip the filtering step if we use the `firsteen:` and `lastseen:`\n",
    "search parameters. Which we do in this example.\n",
    "In the case of Retro Hunt we will query the File Reputation API to retrieve the first seen value for a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c845402da405dc42",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next we want to explore hashes similar to the ones found by Advanced Search.\n",
    "We will define a function `expand_similar()` which will generate PipelineItems with rha1 similarity using the previously defined `rha1_similar_to_sample()`.\n",
    "This function will iterate over the items in the argument.\n",
    "Each item is yielded and then any similar items are yielded after it.\n",
    "This behaviour is similar to a depth first search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac42de7b0f2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_similar(items: Iterable[PipelineItem]):\n",
    "    for i in items:\n",
    "        yield i\n",
    "        stream_of_similar_samples = islice(rha1_similar_to_sample(i.get_sha1()), SIMILAR_LIMIT)\n",
    "        for similar in stream_of_similar_samples:\n",
    "            yield similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a6c2171b197bc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The final step before composing our pipeline is to add the Filtering capability.\n",
    "We define the `PipelineFilter` class.\n",
    "Note that this class contains some statistics as well.\n",
    "Since we will be looking for similar samples it is possible for samples to re-appear in our pipeline (RHA1 similarity).\n",
    "Therefore, we need to keep track of the sample we have already seen.\n",
    "Additionally, we will keep track of samples which we queued for reanalysis so that we can single the out from the YARA matches feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bdd1f66d3be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineFilter:\n",
    "    \n",
    "    def __init__(self, fs_filter, ls_filter):\n",
    "        self.sent_to_reanalysis = set()\n",
    "        self.known = set()\n",
    "        self.fs_filter = fs_filter\n",
    "        self.ls_filter = ls_filter\n",
    "        self.rejected_time = 0\n",
    "        self.rejected_seen = 0\n",
    "        self.call_count = 0\n",
    "    \n",
    "    def never_seen(self, i: PipelineItem):\n",
    "        self.call_count += 1\n",
    "        if self.call_count % 1000 == 0:\n",
    "            print(self.summary())\n",
    "        sha1 = i.get_sha1()\n",
    "        retval = sha1 not in self.known\n",
    "        self.known.add(sha1)\n",
    "        if not retval:\n",
    "            self.rejected_seen += 1\n",
    "        return retval\n",
    "    \n",
    "    def time_gate(self, i: PipelineItem):\n",
    "        self.call_count += 1\n",
    "        if self.call_count % 1000 == 0:\n",
    "            print(self.summary())\n",
    "        retval = self.fs_filter(i) and self.ls_filter(i)\n",
    "        if not retval:\n",
    "            self.rejected_time += 1\n",
    "        return retval\n",
    "    \n",
    "    def newer_than_fs(self, i: PipelineItem):\n",
    "        self.call_count += 1\n",
    "        if self.call_count % 1000 == 0:\n",
    "            print(self.summary())\n",
    "        sha1 = i.get_sha1()\n",
    "        self.known.add(sha1)\n",
    "        retval = self.fs_filter(i)\n",
    "        if not retval:\n",
    "            self.rejected_time += 1\n",
    "        return retval\n",
    "    \n",
    "    def add_reanalyzed(self, hashes: List[str]):\n",
    "        self.call_count += 1\n",
    "        if self.call_count % 1000 == 0:\n",
    "            print(self.summary())\n",
    "        self.sent_to_reanalysis.update(hashes)\n",
    "    \n",
    "    def was_reanalyzed(self, i: PipelineItem):\n",
    "        self.call_count += 1\n",
    "        if self.call_count % 1000 == 0:\n",
    "            print(self.summary())\n",
    "        return i.get_sha1() in self.sent_to_reanalysis\n",
    "    \n",
    "    def summary(self) -> str:\n",
    "        return (f\"Rejected time: {self.rejected_time}\\r\\n\"\n",
    "                f\"Rejected seen: {self.rejected_seen}\\r\\n\"\n",
    "                f\"Reanalyzed hashes: {len(self.sent_to_reanalysis)}\\r\\n\"\n",
    "                f\"Unique hashes: {len(self.known)}\\r\\n\\r\\n\")\n",
    "    \n",
    "\n",
    "pipeline_filter = PipelineFilter(\n",
    "    firstseen_newer_than(year_ago), \n",
    "    lastseen_older_than(ninety_days_ago)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0bef9ce337f9ef",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We compose the generators, and filter to create the `search_and_rha1_stream`.\n",
    "This is the part of the pipeline illustrated by the Stage B on the sketch.\n",
    "We consume items from this stream in batches of 100, since this is the upper bound for the bulk reanalysis request.\n",
    "Note that depending on your Advanced Search query and the limits you set this part of the pipeline may take some time.\n",
    "So take a break while this runs.\n",
    "\n",
    "Note that it will take some time for TiCloud to reanalyze all the sample this stage of the pipeline submits.\n",
    "This part is represented by the dotted circle marked 1 on the sketch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a8ea434594e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_and_rha1_stream = filter(\n",
    "    pipeline_filter.time_gate,\n",
    "    group_with_reputation(\n",
    "        filter(\n",
    "            pipeline_filter.never_seen,\n",
    "            expand_similar(\n",
    "                islice(complete_search_stream(search_query), SEARCH_LIMIT)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "for batch in batched(search_and_rha1_stream, 100):\n",
    "    samples = [i.get_sha1() for i in batch]\n",
    "    reanalyze_file.reanalyze_samples(samples)\n",
    "    pipeline_filter.add_reanalyzed(samples)\n",
    "    print(f\"Submitted {len(samples)} samples for reanalysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7915b0311730869",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the final stage of the pipeline we define two enrichment functions.\n",
    "These will query TiCloud report APIs to pull static and dynamic (if one exists) analysis reports for samples.\n",
    "These work on the same principle as other function in this example so we will not go into detail.\n",
    "\n",
    "After the samples have been reanalyzed they will appear in the regular YARA matches feed, stage C on the sketch.\n",
    "**NOTE** that this feed will contain any sample which arrived to TiCloud since the creation of the rule.\n",
    "You may wish to filter out these samples using the `PipelineFilter.was_reanalyzed()` function.\n",
    "**NOTE** that this feed will contain samples who's firstseen timestamp may fall out of our timeframe.\n",
    "We will filter these out.\n",
    "The same is done with the samples caught by the Retro Hunt feed.\n",
    "\n",
    "In either case we will use the previously defined functions `add_static()` and `add_dynamic()` to enrich the items in the pipeline\n",
    "with static and dynamic reports.\n",
    "Finally, we will further decorate the items in the pipeline with their source stream's name, \"RETRO\" and \"YARA\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543e8c489ecd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_static(items: Iterable[PipelineItem]):\n",
    "    for candidates in batched(items, 100):\n",
    "        sha1_to_item = {e.get_sha1(): e for e in candidates}\n",
    "        static = static_analysis.get_analysis_results(list(sha1_to_item.keys())).json()[\"rl\"]\n",
    "        if static.get(\"invalid_hashes\"):\n",
    "            print(\"Invalid hashes in stream\", static[\"invalid_hashes\"])\n",
    "        for analysis in static[\"entries\"]:\n",
    "            analysis_hash = analysis[\"sha1\"]\n",
    "            i = sha1_to_item[analysis_hash]\n",
    "            i.static = analysis\n",
    "            yield i\n",
    "            \n",
    "            \n",
    "def add_dynamic(items: Iterable[PipelineItem]):\n",
    "    for i in items:\n",
    "        try:\n",
    "            i.dynamic = dynamic_analysis.get_dynamic_analysis_results(i.get_sha1()).json()[\"rl\"][\"report\"]\n",
    "        except NotFoundError:\n",
    "            pass\n",
    "        yield i\n",
    "\n",
    "\n",
    "def add_source(s):\n",
    "    def inner(i: PipelineItem):\n",
    "        return s, i\n",
    "    return inner\n",
    "\n",
    "\n",
    "retro_stream = map(\n",
    "    add_source(\"RETRO\"), \n",
    "    add_dynamic(add_static(\n",
    "        filter(\n",
    "            pipeline_filter.newer_than_fs,\n",
    "            islice(\n",
    "                group_with_reputation(\n",
    "                    complete_retro_feed(retro_started)\n",
    "                ), \n",
    "                RETRO_LIMIT\n",
    "            )\n",
    "        )\n",
    "    ))\n",
    ")\n",
    "\n",
    "yara_stream = map(\n",
    "    add_source(\"YARA\"), \n",
    "    add_dynamic(add_static(\n",
    "        islice(\n",
    "            filter(\n",
    "                pipeline_filter.newer_than_fs, \n",
    "                complete_yara_feed(retro_started)\n",
    "            ),\n",
    "            YARA_LIMIT\n",
    "        )\n",
    "    ))\n",
    ")\n",
    "\n",
    "\n",
    "reports_dir = Path(\"./reports\")\n",
    "reports_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(\"report.csv\", \"w\", newline=\"\", buffering=1) as report:\n",
    "    writer = csv.writer(report, delimiter=\" \", quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow((\"source\", \"sha1\", \"sample_type\", \"sample_available\", \"timestamp\", \"file_size\", \"reanalyzed_via_pipeline\"))\n",
    "    for source, item in chain(retro_stream, yara_stream):\n",
    "        yara_report = item.retro if item.retro else item.yara\n",
    "        writer.writerow((\n",
    "            source,\n",
    "            item.get_sha1(),\n",
    "            yara_report.get(\"file_type\"),\n",
    "            item.sample_available(),\n",
    "            yara_report.get(\"timestamp\"),\n",
    "            yara_report.get(\"file_size\"),\n",
    "            \"Reanalyzed\" if pipeline_filter.was_reanalyzed(item) else \"Not reanalyzed\",\n",
    "        ))\n",
    "        with open(reports_dir / f\"{item.get_sha1()}.json\", \"w\") as sample_report:\n",
    "            json.dump(item.as_dict(), sample_report, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8690a866356f6748",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We combine the two streams into a single one using the `itertoosl.chain()` iterator.\n",
    "Each item from the combined stream is serialized and stored as a json file in the `./reports` directory.\n",
    "We create a CSV file which contains a manifest of collected samples."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
