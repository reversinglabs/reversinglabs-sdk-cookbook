{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e41a75ef",
      "metadata": {},
      "source": [
        "# ReversingLabs SDK Advanced Search \n",
        "\n",
        "This notebook demonstrates how to use the ReversingLabs SDK to search and analyze samples using the `AdvancedSearch` and `AdvancedActions` classes. \n",
        "The Advanced Search enables users to filter samples by search criteria submitted in a POST request. A wide range of search keywords is available, and they can be combined using search operators to build advanced queries.\n",
        "Advanced Actions is a class containing advanced and combined actions utilizing various different classes such as Static analysis (TCA-0104) and Dynamic analsis (TCA-0106). \n",
        "Combined together, client can have a comprehensive enriched report, providing single URL, File type or any other supported filter value.\n",
        "\n",
        "Script includes a recursive function to extract URLs from enriched reports.\n",
        "\n",
        "For a similar implementation reference, see the [ReversingLabs SDK Cookbook - TitaniumCloud Search Notebook](https://alt-gitlab.rl.lan/integrations/sdk/reversinglabs-sdk-cookbook/-/blob/main/TitaniumCloud/search.ipynb?ref_type=heads)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8824c3d",
      "metadata": {},
      "source": [
        "#  1. Importing the required classes\n",
        "First, we will import the required API classes from the ticloud module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ab69c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ReversingLabs.SDK.helper import *\n",
        "from ReversingLabs.SDK.ticloud import AdvancedSearch, AdvancedActions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90ab89cf",
      "metadata": {},
      "source": [
        "#  2. Loading the credentials\n",
        "Next, we will load our TitaniumCloud credentials from the local ticloud_credentials.json file.\n",
        "NOTE: Instead of doing this step, you can paste your credentials while creating the Python object in the following step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ed0816",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# Configuration\n",
        "# ---------------------------------------------------\n",
        "SERVER = \"<server>\"\n",
        "USERNAME = \"username\"\n",
        "PASSWORD = \"password\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5beb2d8",
      "metadata": {},
      "source": [
        "# 3. Filter query\n",
        "This code block defines a Python dictionary named payload that sets up the parameters for an API query to the ReversingLabs platform. When running this in a Jupyter Notebook, it forms the basis for the search request by specifying filters, pagination, and the desired response format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "791783de",
      "metadata": {},
      "outputs": [],
      "source": [
        "payload = {\n",
        "    \"query\": [\n",
        "        {\n",
        "            \"name\": \"firstseen\", # Replace or add Lastseen to narrow the search\n",
        "            \"criteria\": \"range\",\n",
        "            \"value\": {\"from\": \"2025-02-20T00:00:00Z\", \"to\": \"*\"} # Replace with the desired date range\n",
        "        },\n",
        "        {\"name\": \"uri\", \"criteria\": \"eq\", \"value\": \"<URL>\"},  # Replace <URL> with the URL to search (example with wildcard https://api.telegram.org/bot*\") \n",
        "        {\"name\": \"type\", \"criteria\": \"eq\", \"value\": \"PE\"}, # Optional: filter by sample type - values available here: https://docs.reversinglabs.com/SpectraIntelligence/API/MalwareHunting/tca-0320/\n",
        "        {\"name\": \"size\", \"criteria\": \"range\", \"value\": {\"from\": 0, \"to\": \"*\"}}, # Optional: filter by file size\n",
        "        {\"name\": \"classification\", \"criteria\": \"in\", \"value\": [\"malicious\", \"suspicious\"]} # Optional: filter by classification - values available: MALICIOUS, SUSPICIOUS, KNOWN, UNKNOWN.\n",
        "    ],\n",
        "    \"page\": 1,\n",
        "    \"records_per_page\": 100,\n",
        "    \"format\": \"json\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956d0a60",
      "metadata": {},
      "source": [
        "# 4. AdvancedSearch class and subclass\n",
        " This part of the code creates a custom subclass named MyAdvancedSearch that extends the AdvancedSearch class from the ReversingLabs SDK. It customizes the search method to accept a JSON array as its query input, which is ideal for constructing complex, multi-criteria queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbdf8739",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class MyAdvancedSearch(AdvancedSearch):\n",
        "    def search(self, query_string, sorting_criteria=None, sorting_order=\"desc\", page_number=\"page_number\", records_per_page=\"records_per_page\"):\n",
        "        url = self._url.format(endpoint=AdvancedSearch._AdvancedSearch__SINGLE_QUERY_ENDPOINT)\n",
        "        post_json = {\n",
        "            \"query\": query_string,\n",
        "            \"page\": page_number,\n",
        "            \"records_per_page\": records_per_page,\n",
        "            \"format\": \"json\"\n",
        "        }\n",
        "        if sorting_criteria:\n",
        "            sorting_expression = f\"{sorting_criteria} {sorting_order}\"\n",
        "            post_json[\"sort\"] = sorting_expression\n",
        "        response = self._post_request(url=url, post_json=post_json)\n",
        "        self._raise_on_error(response)\n",
        "        return response"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c3e299",
      "metadata": {},
      "source": [
        "# 5. Extract URL prefix from the query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067c7c95",
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_url_prefix_from_query(query_array):\n",
        "    for query_item in query_array:\n",
        "        if query_item.get(\"name\") == \"uri\" and query_item.get(\"criteria\") == \"eq\":\n",
        "            url_pattern = query_item.get(\"value\", \"\")\n",
        "            # Remove the wildcard asterisk if present\n",
        "            return url_pattern.replace(\"*\", \"\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7e378c",
      "metadata": {},
      "source": [
        "# 6. Resursive Function\n",
        "This function, recursive_search_for_urls, recursively traverses an object (which may be a dictionary, list, or string) to find any strings that start with a specified prefix (for example, a URL). It collects these matching strings in a list and returns that list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e39a9c90",
      "metadata": {},
      "outputs": [],
      "source": [
        "def recursive_search_for_urls(obj, prefix):\n",
        "    found = []\n",
        "    if isinstance(obj, dict):\n",
        "        for key, value in obj.items():\n",
        "            found.extend(recursive_search_for_urls(value, prefix))\n",
        "    elif isinstance(obj, list):\n",
        "        for item in obj:\n",
        "            found.extend(recursive_search_for_urls(item, prefix))\n",
        "    elif isinstance(obj, str):\n",
        "        if obj.startswith(prefix):\n",
        "            found.append(obj)\n",
        "    return found"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f59c5f7",
      "metadata": {},
      "source": [
        "# 7. Main Execution: Search, Enrichment, and Report Export\n",
        "- The code starts by reading the search query from the payload.\n",
        "- It creates an instance of a custom AdvancedSearch client and uses it to perform an aggregated search.\n",
        "- For each sample found, it collects minimal information (hashes, first/last seen, sample type, file size, classification, and threat name).\n",
        "- It then enriches each sample using AdvancedActions and extracts any URLs that start with <URL> from the enriched report - example could be \"https://api.telegram.org/bot\"\n",
        "- Finally, all records are saved to \"report.json\".\n",
        "- Note: the output message will state \"Error enriching sample <hash>: Not found. No reference was found for this input\" if the filter did not found data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5680ca67",
      "metadata": {},
      "outputs": [],
      "source": [
        "def main():\n",
        "    query_array = payload[\"query\"]\n",
        "    \n",
        "    # Extract URL prefix from query\n",
        "    url_prefix = extract_url_prefix_from_query(query_array)\n",
        "    if not url_prefix:\n",
        "        print(\"Error: Could not find URL pattern in query\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Using URL prefix for search: {url_prefix}\")\n",
        "\n",
        "    # Instantiate AdvancedSearch client.\n",
        "    search_client = MyAdvancedSearch(\n",
        "        host=SERVER,\n",
        "        username=USERNAME,\n",
        "        password=PASSWORD,\n",
        "        verify=True,\n",
        "        proxies=None,\n",
        "        user_agent=\"ReversingLabs-SDK\",\n",
        "        allow_none_return=False\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        results = search_client.search_aggregated(\n",
        "            query_string=query_array,\n",
        "            sorting_criteria=\"firstseen\",\n",
        "            sorting_order=\"desc\",\n",
        "            max_results=100,\n",
        "            records_per_page=payload.get(\"records_per_page\", 100)\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(\"Error during search:\", e)\n",
        "        return\n",
        "\n",
        "    print(f\"Total samples returned: {len(results)}\")\n",
        "    if not results:\n",
        "        print(\"No samples found.\")\n",
        "        return\n",
        "\n",
        "    actions = AdvancedActions(\n",
        "        host=SERVER,\n",
        "        username=USERNAME,\n",
        "        password=PASSWORD,\n",
        "        verify=True,\n",
        "        proxies=None,\n",
        "        user_agent=\"ReversingLabs-SDK\",\n",
        "        allow_none_return=False\n",
        "    )\n",
        "\n",
        "    # Build minimal results with required fields.\n",
        "    minimal_results = []\n",
        "    for sample in results:\n",
        "        sha1 = sample.get(\"sha1\")\n",
        "        if not sha1:\n",
        "            continue\n",
        "\n",
        "        sample_type = sample.get(\"sampletype\") or sample.get(\"filetype\")\n",
        "        minimal_data = {\n",
        "            \"hashes\": {\n",
        "                \"sha1\": sample.get(\"sha1\"),\n",
        "                \"sha256\": sample.get(\"sha256\", \"\"),\n",
        "                \"md5\": sample.get(\"md5\", \"\")\n",
        "            },\n",
        "            \"first_seen\": sample.get(\"firstseen\"),\n",
        "            \"last_seen\": sample.get(\"lastseen\"),\n",
        "            \"sampletype\": sample_type,\n",
        "            \"file_size\": sample.get(\"size\"),\n",
        "            \"classification\": sample.get(\"classification\"),\n",
        "            \"threatname\": sample.get(\"threatname\")\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            enriched_report = actions.enriched_file_analysis(sha1)\n",
        "        except Exception as e:\n",
        "            print(f\"Error enriching sample {sha1}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Use recursive search to capture URLs using the prefix from query\n",
        "        found_urls = recursive_search_for_urls(enriched_report, url_prefix)\n",
        "        minimal_data[\"extracted_urls\"] = list(set(found_urls))  # Deduplicate\n",
        "\n",
        "        minimal_results.append(minimal_data)\n",
        "\n",
        "    # Group samples by each extracted URL.\n",
        "    url_groups = {}\n",
        "    for sample in minimal_results:\n",
        "        for url in sample.get(\"extracted_urls\", []):\n",
        "            if url not in url_groups:\n",
        "                url_groups[url] = []\n",
        "            url_groups[url].append(sample)\n",
        "\n",
        "    # Prepare final output: list of URL groups.\n",
        "    grouped_output = {\"urls\": []}\n",
        "    for url, samples in url_groups.items():\n",
        "        # Also collect all SHA1 hashes for this URL\n",
        "        hashes = [sample[\"hashes\"][\"sha1\"] for sample in samples]\n",
        "        \n",
        "        grouped_output[\"urls\"].append({\n",
        "            \"value\": url,\n",
        "            \"hashes\": hashes,\n",
        "            \"samples\": samples\n",
        "        })\n",
        "\n",
        "    output_file = \"report_grouped_new-5-b.json\"\n",
        "    try:\n",
        "        with open(output_file, \"w\") as f:\n",
        "            json.dump(grouped_output, f, indent=2)\n",
        "        print(f\"Grouped report written to {output_file}\")\n",
        "    except Exception as e:\n",
        "        print(\"Error exporting report:\", e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
